# CoTrain Logstash Configuration
# Log processing pipeline for CoTrain monorepo project

input {
  # Beats input for log shipping
  beats {
    port => 5044
    host => "0.0.0.0"
  }

  # TCP input for direct log shipping
  tcp {
    port => 5000
    codec => json_lines
  }

  # UDP input for syslog
  udp {
    port => 5514
    codec => json
  }

  # HTTP input for webhook logs
  http {
    port => 8080
    codec => json
  }

  # File input for local log files
  file {
    path => "/var/log/cotrain/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => multiline {
      pattern => "^%{TIMESTAMP_ISO8601}"
      negate => true
      what => "previous"
    }
  }

  # Docker logs input
  file {
    path => "/var/lib/docker/containers/*/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => json
    type => "docker"
  }
}

filter {
  # Parse timestamp
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601" ]
    }
  }

  # Parse Docker logs
  if [type] == "docker" {
    json {
      source => "message"
    }
    
    mutate {
      add_field => { "container_id" => "%{[fields][container_id]}" }
      add_field => { "container_name" => "%{[fields][container_name]}" }
      add_field => { "container_image" => "%{[fields][container_image]}" }
    }
  }

  # Parse application logs
  if [service] == "cotrain-frontend" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:log_message}" }
    }
    
    mutate {
      add_field => { "service_type" => "frontend" }
      add_field => { "technology" => "nextjs" }
    }
  }

  if [service] == "cotrain-backend" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{WORD:context}\] %{LOGLEVEL:level}\s+%{GREEDYDATA:log_message}" }
    }
    
    mutate {
      add_field => { "service_type" => "backend" }
      add_field => { "technology" => "nestjs" }
    }
  }

  # Parse Nginx access logs
  if [service] == "nginx" {
    grok {
      match => { "message" => "%{NGINXACCESS}" }
    }
    
    mutate {
      add_field => { "service_type" => "proxy" }
      convert => { "response" => "integer" }
      convert => { "bytes" => "integer" }
      convert => { "responsetime" => "float" }
    }
    
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
  }

  # Parse PostgreSQL logs
  if [service] == "postgres" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{NUMBER:pid}\] %{WORD:level}:\s+%{GREEDYDATA:log_message}" }
    }
    
    mutate {
      add_field => { "service_type" => "database" }
      add_field => { "technology" => "postgresql" }
    }
  }

  # Parse Redis logs
  if [service] == "redis" {
    grok {
      match => { "message" => "%{NUMBER:pid}:%{WORD:role} %{TIMESTAMP_ISO8601:timestamp} %{WORD:level} %{GREEDYDATA:log_message}" }
    }
    
    mutate {
      add_field => { "service_type" => "cache" }
      add_field => { "technology" => "redis" }
    }
  }

  # Extract user information from logs
  if [log_message] =~ /user_id/ {
    grok {
      match => { "log_message" => "user_id=(?<user_id>[a-f0-9\-]+)" }
    }
  }

  # Extract session information
  if [log_message] =~ /session_id/ {
    grok {
      match => { "log_message" => "session_id=(?<session_id>[a-f0-9\-]+)" }
    }
  }

  # Extract trace information
  if [log_message] =~ /trace_id/ {
    grok {
      match => { "log_message" => "trace_id=(?<trace_id>[a-f0-9]+)" }
    }
  }

  # Extract API endpoint information
  if [log_message] =~ /\/api\// {
    grok {
      match => { "log_message" => "(?<api_endpoint>\/api\/[^\s]+)" }
    }
  }

  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }

  # Add environment information
  mutate {
    add_field => { "environment" => "${ENVIRONMENT:production}" }
    add_field => { "cluster" => "cotrain" }
    add_field => { "version" => "${APP_VERSION:unknown}" }
  }

  # Normalize log levels
  if [level] {
    mutate {
      lowercase => [ "level" ]
    }
    
    if [level] in [ "err", "error", "fatal", "panic" ] {
      mutate {
        replace => { "level" => "error" }
      }
    } else if [level] in [ "warn", "warning" ] {
      mutate {
        replace => { "level" => "warning" }
      }
    } else if [level] in [ "info", "information" ] {
      mutate {
        replace => { "level" => "info" }
      }
    } else if [level] in [ "debug", "trace" ] {
      mutate {
        replace => { "level" => "debug" }
      }
    }
  }

  # Add tags based on content
  if [log_message] =~ /(?i)error|exception|fail/ {
    mutate {
      add_tag => [ "error" ]
    }
  }

  if [log_message] =~ /(?i)slow|timeout|latency/ {
    mutate {
      add_tag => [ "performance" ]
    }
  }

  if [log_message] =~ /(?i)security|auth|login|unauthorized/ {
    mutate {
      add_tag => [ "security" ]
    }
  }

  if [log_message] =~ /(?i)blockchain|transaction|wallet/ {
    mutate {
      add_tag => [ "blockchain" ]
    }
  }

  if [log_message] =~ /(?i)training|model|ai|ml/ {
    mutate {
      add_tag => [ "ml" ]
    }
  }

  # Remove sensitive information
  mutate {
    gsub => [
      "log_message", "password=[^\s]+", "password=***",
      "log_message", "token=[^\s]+", "token=***",
      "log_message", "key=[^\s]+", "key=***",
      "log_message", "secret=[^\s]+", "secret=***"
    ]
  }

  # Clean up fields
  mutate {
    remove_field => [ "host", "agent", "ecs", "input", "@version" ]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "cotrain-logs-%{+YYYY.MM.dd}"
    template_name => "cotrain"
    template_pattern => "cotrain-*"
    template => "/usr/share/logstash/templates/cotrain-template.json"
    template_overwrite => true
  }

  # Output errors to separate index
  if "error" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "cotrain-errors-%{+YYYY.MM.dd}"
    }
  }

  # Output security logs to separate index
  if "security" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "cotrain-security-%{+YYYY.MM.dd}"
    }
  }

  # Output to stdout for debugging (remove in production)
  # stdout {
  #   codec => rubydebug
  # }

  # Output to file for backup
  file {
    path => "/var/log/logstash/cotrain-%{+YYYY-MM-dd}.log"
    codec => json_lines
  }

  # Output metrics to monitoring
  if [service_type] {
    statsd {
      host => "statsd"
      port => 8125
      gauge => { "cotrain.logs.%{service_type}.count" => 1 }
    }
  }
}